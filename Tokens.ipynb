{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a5e663-28d2-4027-8c3f-50dd5cea3938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa8b493d-dbab-41ca-ab04-e9e50e19c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0981bb06-4e5b-4eec-84df-47d1d3c1b4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode_single_token_bytes(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "88f6eee2-ef53-4114-b2ee-89cd9d17c747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoded_model_tokens(model_name, character_set=\"utf-8\"):\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    tokens = []\n",
    "    for index in range(0, encoding.max_token_value):\n",
    "        try:\n",
    "            byte_str = encoding.decode_single_token_bytes(index)\n",
    "            tokens.append(byte_str.decode(character_set))\n",
    "        except KeyError:\n",
    "            pass\n",
    "            #print(f\"Missing token for {index}\")\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "            #print(f\"Incorrect unicode token for {byte_str} at {index}.\")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cd371ec7-b663-4721-a24b-753e0e10c7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_into_file(file_path, data):\n",
    "    f = open(file_path, \"w\")\n",
    "    f.write(data)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0d954df0-974e-4fd0-ba9f-e4dea117168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tokens_for_model(model_name, character_set=\"utf-8\", separator=\"\\n\"):\n",
    "    tokens = get_decoded_model_tokens(model_name, character_set)\n",
    "    write_into_file(f\"./data/{model_name}.txt\", separator.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6893b877-305b-4b5b-ac7d-7cb2c2e409dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gpt-3.5-turbo\",\"gpt-4\",\"gpt-4-32k\",\"text-davinci-003\",\"text-davinci-002\",\"text-davinci-001\",\"text-curie-001\",\"text-babbage-001\",\"text-ada-001\",\"davinci\",\"curie\",\"babbage\",\"ada\",\"code-davinci-002\",\"code-davinci-001\",\"code-cushman-002\",\"code-cushman-001\",\"davinci-codex\",\"cushman-codex\",\"text-davinci-edit-001\",\"code-davinci-edit-001\",\"text-embedding-ada-002\",\"text-similarity-davinci-001\",\"text-similarity-curie-001\",\"text-similarity-babbage-001\",\"text-similarity-ada-001\",\"text-search-davinci-doc-001\",\"text-search-curie-doc-001\",\"text-search-babbage-doc-001\",\"text-search-ada-doc-001\",\"code-search-babbage-code-001\",\"code-search-ada-code-001\",\"gpt2\"]\n",
    "# models = [\"gpt2\",\"r50k_base\",\"p50k_base\",\"p50k_edit\",\"cl100k_base\"]\n",
    "for model in models:\n",
    "    write_tokens_for_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afd8b966-9ff2-4e58-a2b8-9946aa744a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Encoding in module tiktoken.core object:\n",
      "\n",
      "class Encoding(builtins.object)\n",
      " |  Encoding(name: 'str', *, pat_str: 'str', mergeable_ranks: 'dict[bytes, int]', special_tokens: 'dict[str, int]', explicit_n_vocab: 'Optional[int]' = None)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getstate__(self) -> 'object'\n",
      " |  \n",
      " |  __init__(self, name: 'str', *, pat_str: 'str', mergeable_ranks: 'dict[bytes, int]', special_tokens: 'dict[str, int]', explicit_n_vocab: 'Optional[int]' = None)\n",
      " |      Creates an Encoding object.\n",
      " |      \n",
      " |      See openai_public.py for examples of how to construct an Encoding object.\n",
      " |      \n",
      " |      Args:\n",
      " |          name: The name of the encoding. It should be clear from the name of the encoding\n",
      " |              what behaviour to expect, in particular, encodings with different special tokens\n",
      " |              should have different names.\n",
      " |          pat_str: A regex pattern string that is used to split the input text.\n",
      " |          mergeable_ranks: A dictionary mapping mergeable token bytes to their ranks. The ranks\n",
      " |              must correspond to merge priority.\n",
      " |          special_tokens: A dictionary mapping special token strings to their token values.\n",
      " |          explicit_n_vocab: The number of tokens in the vocabulary. If provided, it is checked\n",
      " |              that the number of mergeable tokens and special tokens is equal to this number.\n",
      " |  \n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, value: 'object') -> 'None'\n",
      " |  \n",
      " |  decode(self, tokens: 'list[int]', errors: 'str' = 'replace') -> 'str'\n",
      " |      Decodes a list of tokens into a string.\n",
      " |      \n",
      " |      WARNING: the default behaviour of this function is lossy, since decoded bytes are not\n",
      " |      guaranteed to be valid UTF-8. You can control this behaviour using the `errors` parameter,\n",
      " |      for instance, setting `errors=strict`.\n",
      " |      \n",
      " |      ```\n",
      " |      >>> enc.decode([31373, 995])\n",
      " |      'hello world'\n",
      " |      ```\n",
      " |  \n",
      " |  decode_batch(self, batch: 'list[list[int]]', *, errors: 'str' = 'replace', num_threads: 'int' = 8) -> 'list[str]'\n",
      " |      Decodes a batch (list of lists of tokens) into a list of strings.\n",
      " |  \n",
      " |  decode_bytes(self, tokens: 'list[int]') -> 'bytes'\n",
      " |      Decodes a list of tokens into bytes.\n",
      " |      \n",
      " |      ```\n",
      " |      >>> enc.decode_bytes([31373, 995])\n",
      " |      b'hello world'\n",
      " |      ```\n",
      " |  \n",
      " |  decode_bytes_batch(self, batch: 'list[list[int]]', *, num_threads: 'int' = 8) -> 'list[bytes]'\n",
      " |      Decodes a batch (list of lists of tokens) into a list of bytes.\n",
      " |  \n",
      " |  decode_single_token_bytes(self, token: 'int') -> 'bytes'\n",
      " |      Decodes a token into bytes.\n",
      " |      \n",
      " |      NOTE: this will decode all special tokens.\n",
      " |      \n",
      " |      Raises `KeyError` if the token is not in the vocabulary.\n",
      " |      \n",
      " |      ```\n",
      " |      >>> enc.decode_single_token_bytes(31373)\n",
      " |      b'hello'\n",
      " |      ```\n",
      " |  \n",
      " |  decode_tokens_bytes(self, tokens: 'list[int]') -> 'list[bytes]'\n",
      " |      Decodes a list of tokens into a list of bytes.\n",
      " |      \n",
      " |      Useful for visualising tokenisation.\n",
      " |      >>> enc.decode_tokens_bytes([31373, 995])\n",
      " |      [b'hello', b' world']\n",
      " |  \n",
      " |  decode_with_offsets(self, tokens: 'list[int]') -> 'tuple[str, list[int]]'\n",
      " |      Decodes a list of tokens into a string and a list of offsets.\n",
      " |      \n",
      " |      Each offset is the index into text corresponding to the start of each token.\n",
      " |      If UTF-8 character boundaries do not line up with token boundaries, the offset is the index\n",
      " |      of the first character that contains bytes from the token.\n",
      " |      \n",
      " |      This will currently raise if given tokens that decode to invalid UTF-8; this behaviour may\n",
      " |      change in the future to be more permissive.\n",
      " |      \n",
      " |      >>> enc.decode_with_offsets([31373, 995])\n",
      " |      ('hello world', [0, 5])\n",
      " |  \n",
      " |  encode(self, text: 'str', *, allowed_special: \"Union[Literal['all'], AbstractSet[str]]\" = set(), disallowed_special: \"Union[Literal['all'], Collection[str]]\" = 'all') -> 'list[int]'\n",
      " |      Encodes a string into tokens.\n",
      " |      \n",
      " |      Special tokens are artificial tokens used to unlock capabilities from a model,\n",
      " |      such as fill-in-the-middle. So we want to be careful about accidentally encoding special\n",
      " |      tokens, since they can be used to trick a model into doing something we don't want it to do.\n",
      " |      \n",
      " |      Hence, by default, encode will raise an error if it encounters text that corresponds\n",
      " |      to a special token. This can be controlled on a per-token level using the `allowed_special`\n",
      " |      and `disallowed_special` parameters. In particular:\n",
      " |      - Setting `disallowed_special` to () will prevent this function from raising errors and\n",
      " |        cause all text corresponding to special tokens to be encoded as natural text.\n",
      " |      - Setting `allowed_special` to \"all\" will cause this function to treat all text\n",
      " |        corresponding to special tokens to be encoded as special tokens.\n",
      " |      \n",
      " |      ```\n",
      " |      >>> enc.encode(\"hello world\")\n",
      " |      [31373, 995]\n",
      " |      >>> enc.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})\n",
      " |      [50256]\n",
      " |      >>> enc.encode(\"<|endoftext|>\", allowed_special=\"all\")\n",
      " |      [50256]\n",
      " |      >>> enc.encode(\"<|endoftext|>\")\n",
      " |      # Raises ValueError\n",
      " |      >>> enc.encode(\"<|endoftext|>\", disallowed_special=())\n",
      " |      [27, 91, 437, 1659, 5239, 91, 29]\n",
      " |      ```\n",
      " |  \n",
      " |  encode_batch(self, text: 'list[str]', *, num_threads: 'int' = 8, allowed_special: \"Union[Literal['all'], AbstractSet[str]]\" = set(), disallowed_special: \"Union[Literal['all'], Collection[str]]\" = 'all') -> 'list[list[int]]'\n",
      " |      Encodes a list of strings into tokens, in parallel.\n",
      " |      \n",
      " |      See `encode` for more details on `allowed_special` and `disallowed_special`.\n",
      " |      \n",
      " |      ```\n",
      " |      >>> enc.encode_batch([\"hello world\", \"goodbye world\"])\n",
      " |      [[31373, 995], [11274, 16390, 995]]\n",
      " |      ```\n",
      " |  \n",
      " |  encode_ordinary(self, text: 'str') -> 'list[int]'\n",
      " |      Encodes a string into tokens, ignoring special tokens.\n",
      " |      \n",
      " |      This is equivalent to `encode(text, disallowed_special=())` (but slightly faster).\n",
      " |      \n",
      " |      ```\n",
      " |      >>> enc.encode_ordinary(\"hello world\")\n",
      " |      [31373, 995]\n",
      " |  \n",
      " |  encode_ordinary_batch(self, text: 'list[str]', *, num_threads: 'int' = 8) -> 'list[list[int]]'\n",
      " |      Encodes a list of strings into tokens, in parallel, ignoring special tokens.\n",
      " |      \n",
      " |      This is equivalent to `encode_batch(text, disallowed_special=())` (but slightly faster).\n",
      " |      \n",
      " |      ```\n",
      " |      >>> enc.encode_ordinary_batch([\"hello world\", \"goodbye world\"])\n",
      " |      [[31373, 995], [11274, 16390, 995]]\n",
      " |      ```\n",
      " |  \n",
      " |  encode_single_token(self, text_or_bytes: 'Union[str, bytes]') -> 'int'\n",
      " |      Encodes text corresponding to a single token to its token value.\n",
      " |      \n",
      " |      NOTE: this will encode all special tokens.\n",
      " |      \n",
      " |      Raises `KeyError` if the token is not in the vocabulary.\n",
      " |      \n",
      " |      ```\n",
      " |      >>> enc.encode_single_token(\"hello\")\n",
      " |      31373\n",
      " |      ```\n",
      " |  \n",
      " |  encode_with_unstable(self, text: 'str', *, allowed_special: \"Union[Literal['all'], AbstractSet[str]]\" = set(), disallowed_special: \"Union[Literal['all'], Collection[str]]\" = 'all') -> 'tuple[list[int], list[list[int]]]'\n",
      " |      Encodes a string into stable tokens and possible completion sequences.\n",
      " |      \n",
      " |      Note that the stable tokens will only represent a substring of `text`.\n",
      " |      \n",
      " |      See `encode` for more details on `allowed_special` and `disallowed_special`.\n",
      " |      \n",
      " |      This API should itself be considered unstable.\n",
      " |      \n",
      " |      ```\n",
      " |      >>> enc.encode_with_unstable(\"hello fanta\")\n",
      " |      ([31373], [(277, 4910), (5113, 265), ..., (8842,)])\n",
      " |      \n",
      " |      >>> text = \"...\"\n",
      " |      >>> stable_tokens, completions = enc.encode_with_unstable(text)\n",
      " |      >>> assert text.encode().startswith(enc.decode_bytes(stable_tokens))\n",
      " |      >>> assert all(enc.decode_bytes(stable_tokens + seq).startswith(text.encode()) for seq in completions)\n",
      " |      ```\n",
      " |  \n",
      " |  special_tokens_set = <functools.cached_property object>\n",
      " |  token_byte_values(self) -> 'list[bytes]'\n",
      " |      Returns the list of all token byte values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  eot_token\n",
      " |  \n",
      " |  n_vocab\n",
      " |      For backwards compatibility. Prefer to use `enc.max_token_value + 1`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb1089-0231-4c45-8fd8-b9ccccdf8a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "token-map",
   "language": "python",
   "name": "token-map"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
